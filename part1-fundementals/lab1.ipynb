{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244392f7",
   "metadata": {},
   "source": [
    "In this project, I used UV rather than PIP. See the readme.md for more information including how to download UV and then sync it into an application. UV is a high-performance alternative to pip that is designed to be a drop-in replacement for many workflows, offering significantly faster performance for package installation and dependency resolution. It also combines the functionality of pip, venv, and pip-tools into a single tool, with additional features like environment management and modern dependency resolution. Comparisons are as follows: \n",
    "\n",
    "| Feature | pip | uv |\n",
    "|---------|-----|-----|\n",
    "| **Speed** | Standard (Python-based) | 10-100x faster (Rust-based) |\n",
    "| **Installation** | Bundled with Python | Separate install required |\n",
    "| **Package Installation** | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Dependency Resolution** | Basic | Advanced, more reliable |\n",
    "| **Virtual Environments** | Via separate `venv` module | Built-in creation & management |\n",
    "| **Python Version Management** | ‚ùå No | ‚úÖ Yes (installs Python versions) |\n",
    "| **Lock Files** | ‚ùå No (community tools needed) | ‚úÖ Yes (uv.lock) |\n",
    "| **Parallel Downloads** | Limited | ‚úÖ Yes |\n",
    "| **Caching** | Basic | Advanced, cross-project |\n",
    "| **Project Management** | ‚ùå No | ‚úÖ Yes (similar to Poetry) |\n",
    "| **requirements.txt Support** | ‚úÖ Native | ‚úÖ Compatible |\n",
    "| **Maturity** | Established (2000s) | New (2024) |\n",
    "| **Compatibility** | Universal | High, but newer |\n",
    "| **Written In** | Python | Rust |\n",
    "| **Use Case** | General purpose, legacy | Modern projects, speed-critical |\n",
    "| **Learning Curve** | Minimal | Slightly steeper |\n",
    "| **Community Adoption** | Universal standard | Growing rapidly |\n",
    "\n",
    "There was an issue with UV sync because my VS Code environment didn't inherit the windows path. So I went to File\\Preferences\\Settings and searched for terminal.integrated.env.windows. This gave me the option to open this JSON file. You can set terminal.integrated.inheritEnv to true or add the path to terminal.integrated.env.windows as shown below.\n",
    "\n",
    "```jsonc\n",
    "{\n",
    "    \"python.defaultInterpreterPath\": \"c:\\\\Users\\\\damie\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\python.exe\",\n",
    "    \"workbench.editor.empty.hint\": \"hidden\",\n",
    "    \"explorer.compactFolders\": false,\n",
    "    \"git.autofetch\": true,\n",
    "    \"terminal.integrated.env.windows\": {\n",
    "        \"PATH\": \"${env:PATH}\"\n",
    "    },\n",
    "    \"terminal.integrated.inheritEnv\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96520fca",
   "metadata": {},
   "source": [
    "# Basic Agentic Workflow\n",
    "\n",
    "Helloooo everyone and welcome to an exciting lesson on Agentic AI! üéâ\n",
    "\n",
    "Today, we're diving into **prompt chaining** agentic workflow pattern. What's that? It's just passing the output from one LLM to the next, step by step. Think of it like a relay race, but with prompts instead of batons.\n",
    "\n",
    "We'll keep things super simple: manually run each cell, watch the magic happen, and see how chaining LLM calls lets us build more complex workflows.\n",
    "\n",
    "Ready to see how agents can work together? Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eac71",
   "metadata": {},
   "source": [
    "## As always, libraries first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f696afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# check if API keys are set\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing OpenAI API key\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"Missing Gemini API key\")\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    raise ValueError(\"Missing Anthropic API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab264f",
   "metadata": {},
   "source": [
    "You can set your API Keys for each of the LLM providers using the following links:\n",
    "\n",
    "- [OpenAI](https://platform.openai.com/api-keys)\n",
    "- [Anthropic](https://console.anthropic.com/settings/keys)\n",
    "- [Gemini](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "Once you have created the API Keys, you can store them on your `.env` file at the root of this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91357479",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:16px;background:#2e3440;margin:1em 0;padding:1em 1em 1em 3em;color:#eceff4;position:relative;box-shadow:0 6px 16px rgba(0,0,0,.4)\">\n",
    "    <b style=\"color:#88c0d0;font-size:1.25em\">Info:</b>\n",
    "    <ul style=\"margin:.6em 0 0;padding-left:1.2em;line-height:1.6\">\n",
    "        <li>You can complete this entire notebook using just OpenAI models if you prefer!</li>\n",
    "        <li>It's absolutely fine to skip Anthropic and Gemini for now ‚Äî the workflow works perfectly with only OpenAI.</li>\n",
    "        <li>Feel free to experiment with other providers later, but don't let missing API keys slow you down.</li>\n",
    "    </ul>\n",
    "    <div style=\"position:absolute;top:-.8em;left:-.8em;width:2.4em;height:2.4em;border-radius:50%;background:#88c0d0;color:#2e3440;display:flex;align-items:center;justify-content:center;font-weight:700;font-size:1.2em\">üí°</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ed0b8",
   "metadata": {},
   "source": [
    "## The Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01562382",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A[Generate Tickets] --> B[Classify Priority] --> C[Respond to Tickets]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8641ab",
   "metadata": {},
   "source": [
    "## Lets start with using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24530e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ab44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages list\n",
    "message = \"I want you to generate a customer support ticket for a 3rd party re-seller. \"\n",
    "message += \"The ticket should be a single sentence describing a common issue a customer might face with their product or service. \"\n",
    "message += \"Please ensure the ticket is varied and covers different types of problems. \"\n",
    "message += \"Do not include a subject, only the body of the ticket.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa74d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Ticket:\n",
       "The customer is unable to activate the software license due to an error message during installation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response\n",
    "\n",
    "openai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "ticket = openai_response.choices[0].message.content\n",
    "# print(f\"### Generated Ticket:\\n{ticket}\")\n",
    "display(Markdown(f\"### Generated Ticket:\\n{ticket}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec94e3",
   "metadata": {},
   "source": [
    "I love markdown. It is a lightweight method of rendering and formatting text that is super versatile without having to use heavy softwares like MS Word or Google Docs.\n",
    "\n",
    "You can learn more about markdown sytanx [here](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "A really informative YouTube video talking about the [Unreasonable Effectiveness of Plain Text](https://www.youtube.com/watch?v=WgV6M1LyfNY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38015b29",
   "metadata": {},
   "source": [
    "## Lets pass these on to an Anthropic model and ask it to classify the priority level of each ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic client. Note we are using the OpenAI wrapper for Anthropic by passing in the base_url\n",
    "anthropic_client = OpenAI(api_key=ANTHROPIC_API_KEY, base_url=\"https://api.anthropic.com/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4affb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages list\n",
    "message = \"I want you to classify the priority of the following customer support ticket. \"\n",
    "message += \"The ticket is as follows: \"+ ticket + \" \"\n",
    "message += \"Please classify the priority as either 'Low', 'Medium', or 'High'. \"\n",
    "message += \"Respond with only the priority level.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e7c7c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Classified Priority:\n",
       "High"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response\n",
    "\n",
    "anthropic_response = anthropic_client.chat.completions.create(\n",
    "    model=\"claude-3-5-haiku-latest\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "priority = anthropic_response.choices[0].message.content\n",
    "display(Markdown(f\"### Classified Priority:\\n{priority}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed235e15",
   "metadata": {},
   "source": [
    "## Now Gemini should determine the appropriate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2304b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini client. Note we are using the OpenAI wrapper for Gemini by passing in the base_url\n",
    "gemini_client = OpenAI(api_key=GEMINI_API_KEY, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b123501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages list\n",
    "\n",
    "message = \"You are to determine an appropriate response to the following customer support ticket. \"\n",
    "message += \"The ticket is as follows: \"+ ticket + \" \"\n",
    "message += \"The priority level of this ticket is: \" + priority + \" \"\n",
    "message += \"Please provide a response that addresses the customer's issue in a short and concise manner. \"\n",
    "    \n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3addbecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Generated Response:\n",
       "Subject: High Priority: Urgent Assistance Needed for License Activation Error\n",
       "\n",
       "Dear [Customer Name],\n",
       "\n",
       "We understand you're experiencing a high-priority issue with your software license activation during installation, and we apologize for the inconvenience this is causing.\n",
       "\n",
       "To help us resolve this swiftly, please provide the exact error message you are receiving. Our technical team is ready to provide immediate support and will contact you directly with specific next steps, which may include a call or remote session.\n",
       "\n",
       "Sincerely,\n",
       "[Your Name/Support Team]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response\n",
    "\n",
    "gemini_response = gemini_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "response = gemini_response.choices[0].message.content\n",
    "display(Markdown(f\"### Generated Response:\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be180d46",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:16px;background:#1e2a1e;margin:1em 0;padding:1em 1em 1em 3em;color:#eceff4;position:relative;box-shadow:0 6px 16px rgba(0,0,0,.4)\">\n",
    "    <b style=\"color:#a3be8c;font-size:1.25em\">Your Challenge:</b>\n",
    "    <ul style=\"margin:.6em 0 0;padding-left:1.2em;line-height:1.6\"></ul>\n",
    "        <li>Recreate the customer support ticket workflow using an <b>evaluator-optimizer agentic workflow pattern</b> instead of prompt chaining.</li>\n",
    "        <li>Your evaluator agent should assess the quality and completeness of each ticket and suggest improvements.</li>\n",
    "        <li>Your optimizer agent should revise the tickets based on evaluator feedback, aiming for clarity and actionable details.</li>\n",
    "        <li>Try to implement this using at least two LLM calls (one for evaluation, one for optimization) and display the before/after results.</li>\n",
    "        <li>Share your work in the community-contributions folder by creating a folder with your name. Eg. shaheer-airaj.</li>\n",
    "    </ul>\n",
    "    <div style=\"position:absolute;top:-.8em;left:-.8em;width:2.4em;height:2.4em;border-radius:50%;background:#a3be8c;color:#2e3440;display:flex;align-items:center;justify-content:center;font-weight:700;font-size:1.2em\">üí™</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
