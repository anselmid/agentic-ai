{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a650a62",
   "metadata": {},
   "source": [
    "# Function Calling/Tool Use\n",
    "\n",
    "\n",
    "## Just a quick definition\n",
    "\n",
    "Function calling in the context of Large Language Models (LLMs) refers to the ability of the model to invoke external functions or tools during its response generation. Instead of only generating text, the LLM can recognize when a task requires external data or computation, call a predefined function with the appropriate arguments, and then use the function's output to continue its response.\n",
    "\n",
    "Think of having a super smart robot that you can only speak with. Pretty useless at doing anything aside from talking. Now imagine you have given that robot a hammer and some nails. It can now put up that wall painting that's been waiting forever to be hung :D\n",
    "\n",
    "Except, the way we as AI Engineers give AI tools is by defining python functions and describing that function in detail to the LLM so it knows what tools it has access to, what each tool can do, what are its inputs and expected outputs.\n",
    "\n",
    "Just remember, a *tool* in its simplest form is just a *python function* that you have defined. It can be as simple as a calculator function or something like being able to call external APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f0cfa",
   "metadata": {},
   "source": [
    "![tool-use](../images/tool-use.png)\n",
    "\n",
    "*Image courtesy of [API Deck](https://www.apideck.com/blog/llm-tool-use-and-function-calling)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa669de",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bffb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise Exception(\"API key is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d86cc",
   "metadata": {},
   "source": [
    "## Step 2: Define a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3997d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfddda44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weather(24.343627, 54.497922)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b3e24",
   "metadata": {},
   "source": [
    "## Step 3: Call a chat model normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cc1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide real-time weather updates. For the most current weather in Abu Dhabi, I recommend checking a reliable weather website or app such as Weather.com, AccuWeather, or a local weather service.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# The chat.completions endpoint is the traditional way to get responses from models like ChatGPT, requiring you to manage conversation \n",
    "# history by sending the full message list with each request. In contrast, the newer Responses API is stateful, meaning it manages \n",
    "# history internally based on a conversation ID, which simplifies development and is optimized for agent-like workflows with built-in \n",
    "# tools like web search and function calling. While both can be used for chat, the Responses API is recommended for new projects due \n",
    "# to its built-in features and simpler history management, though chat.completions will still be supported. Note the input= parameter \n",
    "# in the Responses API\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Abu Dhabi now?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7d64a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_0b5dab7e9760cd54006920eed9e40c81a2b6532660786fea97',\n",
       " 'created_at': 1763765977.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-nano-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [ResponseOutputMessage(id='msg_0b5dab7e9760cd54006920eeda8b8081a28bb71cba0eb8b150', content=[ResponseOutputText(annotations=[], text=\"I'm unable to provide real-time weather updates. For the most current weather in Abu Dhabi, I recommend checking a reliable weather website or app such as Weather.com, AccuWeather, or a local weather service.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'conversation': None,\n",
       " 'max_output_tokens': None,\n",
       " 'max_tool_calls': None,\n",
       " 'previous_response_id': None,\n",
       " 'prompt': None,\n",
       " 'prompt_cache_key': None,\n",
       " 'reasoning': Reasoning(effort=None, generate_summary=None, summary=None),\n",
       " 'safety_identifier': None,\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),\n",
       " 'top_logprobs': 0,\n",
       " 'truncation': 'disabled',\n",
       " 'usage': ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=43, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=60),\n",
       " 'user': None,\n",
       " '_request_id': 'req_4e6ca2a32c69420eaaf8bda0a4dc91b9'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__\n",
    "#Note the response id at the top and the output. Also note the tools  object which is empty as we have not tool in use yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c75fa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_0b5dab7e9760cd54006920eeda8b8081a28bb71cba0eb8b150',\n",
       " 'content': [ResponseOutputText(annotations=[], text=\"I'm unable to provide real-time weather updates. For the most current weather in Abu Dhabi, I recommend checking a reliable weather website or app such as Weather.com, AccuWeather, or a local weather service.\", type='output_text', logprobs=[])],\n",
       " 'role': 'assistant',\n",
       " 'status': 'completed',\n",
       " 'type': 'message'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[0].__dict__\n",
    "# Looking at the type = message. This will change to function when we use tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ab349",
   "metadata": {},
   "source": [
    "## Step 4: Define the input schema for our tool\n",
    "\n",
    "Lets try giving our `get_weather` function as a tool to our AI.\n",
    "\n",
    "We will first need to define the tool in a format OpenAI can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ead465",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'name': 'get_weather',\n",
    "    'description': 'Fetch the current weather for a specific location',\n",
    "    'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'latitude': {'type': 'number'},\n",
    "            'longitude': {'type': 'number'}\n",
    "        },\n",
    "        'required': ['latitude', 'longitude'],\n",
    "        'additionalProperties': False\n",
    "    },\n",
    "    'strict': True\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9f2e8",
   "metadata": {},
   "source": [
    "## Step 5: Pass the tool schema over to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce5e10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Abu Dhabi now?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecd57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=input_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651e424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)\n",
    "# there is no output yet as the model has decided to call a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe27b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_02141e41f4dcc3bb006920ef2141148192a0ace01ee8523628',\n",
       " 'created_at': 1763766049.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-nano-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [ResponseFunctionToolCall(arguments='{\"latitude\":24.4539,\"longitude\":54.3773}', call_id='call_b5c0aDreSDbhw4ypQCZHkrSv', name='get_weather', type='function_call', id='fc_02141e41f4dcc3bb006920ef2250bc81928d5e011604a9d164', status='completed')],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'latitude': {'type': 'number'}, 'longitude': {'type': 'number'}}, 'required': ['latitude', 'longitude'], 'additionalProperties': False}, strict=True, type='function', description='Fetch the current weather for a specific location')],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'conversation': None,\n",
       " 'max_output_tokens': None,\n",
       " 'max_tool_calls': None,\n",
       " 'previous_response_id': None,\n",
       " 'prompt': None,\n",
       " 'prompt_cache_key': None,\n",
       " 'reasoning': Reasoning(effort=None, generate_summary=None, summary=None),\n",
       " 'safety_identifier': None,\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),\n",
       " 'top_logprobs': 0,\n",
       " 'truncation': 'disabled',\n",
       " 'usage': ResponseUsage(input_tokens=54, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=41, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=95),\n",
       " 'user': None,\n",
       " '_request_id': 'req_50c618c56c944dc4bf61ba94cef48bd8'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__\n",
    "# notice the output is a ResponseFunctionCall object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49ba891f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': '{\"latitude\":24.4539,\"longitude\":54.3773}',\n",
       " 'call_id': 'call_b5c0aDreSDbhw4ypQCZHkrSv',\n",
       " 'name': 'get_weather',\n",
       " 'type': 'function_call',\n",
       " 'id': 'fc_02141e41f4dcc3bb006920ef2250bc81928d5e011604a9d164',\n",
       " 'status': 'completed'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[0].__dict__\n",
    "# Inspecting the output object for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fea85f",
   "metadata": {},
   "source": [
    "## Step 6: Format the tool call response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4619e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Extracting the tool call and its arguments\n",
    "tool_call = response.output[0]\n",
    "# the arguments are in JSON format\n",
    "args = json.loads(tool_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbeef1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFunctionToolCall(arguments='{\"latitude\":24.4539,\"longitude\":54.3773}', call_id='call_b5c0aDreSDbhw4ypQCZHkrSv', name='get_weather', type='function_call', id='fc_02141e41f4dcc3bb006920ef2250bc81928d5e011604a9d164', status='completed')\n"
     ]
    }
   ],
   "source": [
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68f003b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 24.4539, 'longitude': 54.3773}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ff324",
   "metadata": {},
   "source": [
    "## Step 7: Pass on the tool call arguments to our tool/python function\n",
    "\n",
    "We now need to pass on the arguments received by the model to our python function or tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec267d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_weather(args['latitude'], args['longitude'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1829435",
   "metadata": {},
   "source": [
    "## Step 8: Append the response of the tool into the message list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input messages already has the original question (what is the weather like in Abu Dhabi now? )\n",
    "# Now add the definition of the tool call chatgpt asked us to call\n",
    "input_messages.append(tool_call)\n",
    "# Then add the output of the tool call. The call id links the output to the specific tool call\n",
    "input_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": tool_call.call_id,\n",
    "    \"output\": str(result)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298efe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'What is the weather like in Abu Dhabi now?', 'role': 'user'},\n",
      " ResponseFunctionToolCall(arguments='{\"latitude\":24.4539,\"longitude\":54.3773}', call_id='call_b5c0aDreSDbhw4ypQCZHkrSv', name='get_weather', type='function_call', id='fc_02141e41f4dcc3bb006920ef2250bc81928d5e011604a9d164', status='completed'),\n",
      " {'call_id': 'call_b5c0aDreSDbhw4ypQCZHkrSv',\n",
      "  'output': '23.9',\n",
      "  'type': 'function_call_output'}]\n"
     ]
    }
   ],
   "source": [
    "# use pretty print to see the input messages a bit clearer\n",
    "from pprint import pprint\n",
    "pprint(input_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a76b0c",
   "metadata": {},
   "source": [
    "## Step 9: Pass the message list into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2444c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input=input_messages,\n",
    "    tools=tools\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fcba22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Abu Dhabi is approximately 23.9Â°C.\n"
     ]
    }
   ],
   "source": [
    "print(response_2.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f2b735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_02141e41f4dcc3bb006920ef9c863081928f06a807c1b37d65',\n",
       " 'created_at': 1763766172.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-nano-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [ResponseOutputMessage(id='msg_02141e41f4dcc3bb006920ef9cf6008192916c9384b1601c0b', content=[ResponseOutputText(annotations=[], text='The current temperature in Abu Dhabi is approximately 23.9Â°C.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'latitude': {'type': 'number'}, 'longitude': {'type': 'number'}}, 'required': ['latitude', 'longitude'], 'additionalProperties': False}, strict=True, type='function', description='Fetch the current weather for a specific location')],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'conversation': None,\n",
       " 'max_output_tokens': None,\n",
       " 'max_tool_calls': None,\n",
       " 'previous_response_id': None,\n",
       " 'prompt': None,\n",
       " 'prompt_cache_key': None,\n",
       " 'reasoning': Reasoning(effort=None, generate_summary=None, summary=None),\n",
       " 'safety_identifier': None,\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),\n",
       " 'top_logprobs': 0,\n",
       " 'truncation': 'disabled',\n",
       " 'usage': ResponseUsage(input_tokens=87, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=16, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=103),\n",
       " 'user': None,\n",
       " '_request_id': 'req_21f0a561af0f46c98d6bad847b8730ff'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30de53b",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "- [OpenAIs Function Calling Guide](https://platform.openai.com/docs/guides/function-calling?api-mode=responses)\n",
    "- [Anthropics Tool Use Guide with Claude](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview)\n",
    "- [Function Calling with Gemini API](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc4aab",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:16px;background:#1e2a1e;margin:1em 0;padding:1em 1em 1em 3em;color:#eceff4;position:relative;box-shadow:0 6px 16px rgba(0,0,0,.4)\">\n",
    "  <b style=\"color:#a3be8c;font-size:1.25em\">Your Challenge:</b>\n",
    "  <div style=\"position:absolute;top:-.8em;left:-.8em;width:2.4em;height:2.4em;border-radius:50%;background:#a3be8c;color:#2e3440;display:flex;align-items:center;justify-content:center;font-weight:700;font-size:1.2em\">ðŸ’ª</div>\n",
    "\n",
    "  <br>\n",
    "  Now that you've seen how to define a Python function as a tool and connect it to an LLM, it's time to get creative!\n",
    "\n",
    "  <b>Pick your challenge</b><br>\n",
    "  <b>1. Wikipedia Article Summarizer</b><br>\n",
    "  - Fetch a wikipedia article you are interested in learning about and have an LLM summarize it<br>\n",
    "  - <a href=\"https://pypi.org/project/Wikipedia-API/\">Wikipedia API</a><br>\n",
    "\n",
    "  <b>2. News Summarizer</b><br>\n",
    "  - Fetch latest headlines or news by topic.<br>\n",
    "  - <a href=\"https://newsapi.org\">NewsAPI</a><br>\n",
    "  - <a href=\"https://currentsapi.services/en/docs/\">CurrentsAPI</a><br>\n",
    "\n",
    "  <b>3. Stock Market Data</b><br>\n",
    "  - Get real-time stock prices or company info<br>\n",
    "  - <a href=\"https://www.alphavantage.co/\">Alpha Vantage</a><br>\n",
    "  - <a href=\"https://pypi.org/project/yfinance/\">yfinance</a><br>\n",
    "\n",
    "  <b>4. Movie Info</b><br>\n",
    "  - Get movie ratings, cast, plot summaries.<br>\n",
    "  - <a href=\"https://imdbapi.dev/\">IMDb</a><br>\n",
    "  - <a href=\"https://www.omdbapi.com/\">OMDb API</a><br>\n",
    "\n",
    "  <b>5. NASA API</b><br>\n",
    "  - Astronomy facts, country data, etc.<br>\n",
    "  - <a href=\"https://api.nasa.gov/\">NASA APIs</a><br>\n",
    "\n",
    "  <b>6. Your own custom tool</b><br>\n",
    "  - Think of a real-world use case where an LLM could benefit from calling a custom function.<br>\n",
    "\n",
    "  <hr>\n",
    "\n",
    "  <b>Tips</b>:<br>\n",
    "  - Use clear function names and docstrings.<br>\n",
    "  - Handle input arguments and outputs carefully.<br>\n",
    "  - Print the LLM's tool call and your function's output in a readable way.<br>\n",
    "\n",
    "  Be sure to place your submissions in <code>part1-fundamentals/community-contributions/&lt;your-name&gt;</code><br>\n",
    "\n",
    "  I'm super excited to see what you come up with :D\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
